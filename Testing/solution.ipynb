{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chubby-tenant",
   "metadata": {},
   "source": [
    "## PHME 2022 Data Challenge\n",
    "\n",
    "This is the skeleton of the Jupyter Notebook you have to fill.\n",
    "The Notebook must define three functions, each for solving a separate classification task of the challenge.\n",
    "They are `classification_1`, `classification_2` and `classification_3` and must solve, respectively, task 1, 2 and 3 of the challenges.\n",
    "\n",
    "**Automatic Scoring and Leader Board:** \n",
    "For each participant, we will consider the file locate in `data-challenge-phme/solution.ipynb` as proposed solution. We will execute the Notebook and, in the end, invoke the functions with the test data.\n",
    "We will evaluate the output of the functions and compute the performance on it.\n",
    "\n",
    "**Note:** if the execution of the notebook leads to an error or an exception, the functions will not be defined and we will not evaluate the performance of your solution.\n",
    "\n",
    "**Note:** the notebook must have a reasonable execution time. We will not evaluate notebooks requiring more than **10 minutes** to be executed.\n",
    "If you want to train complex models that require a large amount of time, do it in a separate notebook. Thus, in `solution.ipynb`, load pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mature-philippines",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.2/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm \n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "import random\n",
    "import xgboost\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "promotional-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input is the SPI data in form of a Pandas DataFrame, exactly as it is read with pd.read_csv()\n",
    "# The output must be the list of predicted defects. Each defect is a tuple (Panel, Figure, Component)\n",
    "\n",
    "def classification_1 (spi):\n",
    "    spi_real=spi.copy()\n",
    "    spi_real=spi_real.dropna()\n",
    "    spi_real[\"ComponentID2\"]=spi_real[\"ComponentID\"]\n",
    "    spi_real[\"FigureID2\"]=spi_real[\"FigureID\"]\n",
    "    spi_real[\"Result\"]=spi_real[\"Result\"].astype(\"category\").cat.codes\n",
    "    Num_features_to_cat=['Volume(%)','Area(%)', 'OffsetX(%)','OffsetY(%)']\n",
    "    Num_features=['Shape(um)',\n",
    "       'PosX(mm)', 'PosY(mm)','SizeX', 'SizeY']\n",
    "    for feat in Num_features:\n",
    "        spi_real[feat]=spi_real[feat].astype('float')\n",
    "    encoder=joblib.load(\"Cat_boost_step1\")\n",
    "    \n",
    "    spi_real[\"ComponentID2\"]=spi_real[\"ComponentID\"].astype('category').cat.codes\n",
    "\n",
    "    list_var_to_encode=[\"ComponentID2\"]\n",
    "    \n",
    "    AE_val=encoder.transform(spi_real[list_var_to_encode].copy())\n",
    "    AE_val.columns=[x+\"_encoded\" for x in AE_val.columns.tolist()]\n",
    "    spi_real=pd.concat([spi_real,AE_val],axis=1)\n",
    "    \n",
    "    features_x=[\"ComponentID2_encoded\"]+Num_features_to_cat+Num_features\n",
    "    X_test=spi_real[features_x].to_numpy()\n",
    "    \n",
    "    clf = joblib.load(\"xgboost_step1\")\n",
    "    \n",
    "    pred_xgboost=clf.predict(X_test)\n",
    "    spi_real['Pred_label']=pred_xgboost\n",
    "\n",
    "    spi_real=spi_real.loc[spi_real['Pred_label']==0]\n",
    "    spi_real=spi_real.dropna(subset=['FigureID2'])\n",
    "\n",
    "    spi_real[\"FigureID2\"]=spi_real[\"FigureID2\"].astype(int).astype(str)\n",
    "\n",
    "    defects=list(spi_real[[\"PanelID\",\"FigureID2\",\"ComponentID\"]].itertuples(index=False, name=None))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    return defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "received-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first input is the SPI data in form of a Pandas DataFrame, exactly as it is read with pd.read_csv()\n",
    "# The second input is the AOI data. OperatorLabel and RepairLabel are not included, as you must predict OperatorLabel\n",
    "# The output must be the classification result. Each entry is a tuple (Panel, Figure, Component, PredictedOperatorLabel)\n",
    "\n",
    "def classification_2 (spi, aoi):\n",
    "    spi_real=spi.copy()\n",
    "    aoi_real=aoi.copy()\n",
    "    spi_real['PinNumber']=spi_real['PinNumber']\n",
    "    aoi_real['PinNumber']=aoi_real['PinNumber'].astype('Int64').astype(str)\n",
    "    aoi_real['FigureID2']=aoi_real['FigureID']\n",
    "    aoi_real['ComponentID2']=aoi_real['ComponentID']\n",
    "    aoi_real['Count_Pin']=aoi_real.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])[\"PinNumber\"].transform('count')\n",
    "    aoi_real['Count_Pin_Figure']=aoi_real.groupby([\"PanelID\",\"FigureID2\"])[\"PinNumber\"].transform('count')\n",
    "    aoi_real['Count_Pin_Panel']=aoi_real.groupby([\"PanelID\"])[\"PinNumber\"].transform('count')\n",
    "\n",
    "\n",
    "    aoi_real['Count_Pin']=aoi_real.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])[\"PinNumber\"].transform('count')\n",
    "    aoi_real['Count_Pin_Figure']=aoi_real.groupby([\"PanelID\",\"FigureID2\"])[\"PinNumber\"].transform('count')\n",
    "\n",
    "    spi_real=spi_real.dropna()\n",
    "\n",
    "\n",
    "\n",
    "    list_var_to_encode=[\"AOILabel\",\"ComponentID\",\"FigureID_ComponentID\"]\n",
    "    Num_features_to_cat=['Volume(%)','Area(%)', 'OffsetX(%)','OffsetY(%)']\n",
    "    Num_features=['Shape(um)',\n",
    "       'PosX(mm)', 'PosY(mm)','SizeX', 'SizeY']    \n",
    "    aoi_real['AOILabel2']=aoi_real['AOILabel']\n",
    "\n",
    "   \n",
    "    aoi_inner=aoi_real.merge(spi_real,on=['PanelID','FigureID',\"ComponentID\",\"PinNumber\"],how=\"inner\")\n",
    "\n",
    "    aoi_inner['FigureID_ComponentID']=(aoi_inner['FigureID'].astype(str)+'_'+aoi_inner['ComponentID'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_inner['FigureID_ComponentID_PinNumber']=(aoi_inner['FigureID'].astype(str)+'_'+aoi_inner['ComponentID'].astype(str)+'_'+aoi_inner['PinNumber'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_inner['FigureID_ComponentID_PinNumber_AOILabel']=(aoi_inner['FigureID'].astype(str)+'_'+aoi_inner['ComponentID'].astype(str)+'_'+aoi_inner['PinNumber'].astype(str)+aoi_inner['AOILabel'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_inner['FigureID_ComponentID_AOILabel']=(aoi_inner['FigureID'].astype(str)+'_'+aoi_inner['ComponentID'].astype(str)+\"_\"+aoi_inner['AOILabel'].astype(str)).astype(\"category\").cat.codes\n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    list_var_to_encode=[\"AOILabel\",\"ComponentID\",\"FigureID_ComponentID\"]\n",
    "    Num_features_to_cat=['Volume(%)','Area(%)', 'OffsetX(%)','OffsetY(%)']\n",
    "    Num_features=['Shape(um)',\n",
    "       'PosX(mm)', 'PosY(mm)','SizeX', 'SizeY']    \n",
    "    \n",
    "    for var in list_var_to_encode:\n",
    "        aoi_inner[var]=aoi_inner[var].astype('category').cat.codes\n",
    "\n",
    "    encoder=joblib.load(\"Cat_boost_step2_inner\")\n",
    "    AE_val=encoder.transform(aoi_inner[list_var_to_encode].copy())\n",
    "    AE_val.columns=[x+\"_encoded\" for x in AE_val.columns.tolist()]\n",
    "    aoi_inner=pd.concat([aoi_inner,AE_val],axis=1)\n",
    "\n",
    "    col_features=[x+\"_encoded\" for x in list_var_to_encode]+Num_features+Num_features_to_cat+[\"Count_Pin\",\"Count_Pin_Figure\"]\n",
    "\n",
    "\n",
    "    aoi_inner=aoi_inner.replace(np.nan,0)\n",
    "    X_test_final=aoi_inner[col_features].to_numpy()\n",
    "\n",
    "    xgb_cl=joblib.load(\"xgboost_step2_inner\")\n",
    "    aoi_inner['Pred']=xgb_cl.predict(X_test_final)\n",
    "    \n",
    "    spi_real=spi.copy()\n",
    "    spi_real=spi_real.dropna()\n",
    "\n",
    "\n",
    "    Features_to_bin=['Volume(%)',  'Area(%)', 'OffsetX(%)','OffsetY(%)']\n",
    "\n",
    "    InputSPI_grouped=spi_real.groupby([\"PanelID\",\"FigureID\",\"ComponentID\"])[Features_to_bin].mean().reset_index()\n",
    "    \n",
    " \n",
    "    \n",
    "    aoi_outter=aoi_real.loc[aoi_real.PinNumber=='<NA>']\n",
    "\n",
    "    aoi_outter=aoi_outter.merge(InputSPI_grouped,on=['PanelID','FigureID',\"ComponentID\"],how=\"left\")\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    aoi_outter['FigureID_ComponentID']=(aoi_outter['FigureID'].astype(str)+'_'+aoi_outter['ComponentID'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_outter['FigureID_ComponentID_PinNumber']=(aoi_outter['FigureID'].astype(str)+'_'+aoi_outter['ComponentID'].astype(str)+'_'+aoi_outter['PinNumber'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_outter['FigureID_ComponentID_PinNumber_AOILabel']=(aoi_outter['FigureID'].astype(str)+'_'+aoi_outter['ComponentID'].astype(str)+'_'+aoi_outter['PinNumber'].astype(str)+aoi_outter['AOILabel'].astype(str)).astype(\"category\").cat.codes\n",
    "    aoi_outter['FigureID_ComponentID_AOILabel']=(aoi_outter['FigureID'].astype(str)+'_'+aoi_outter['ComponentID'].astype(str)+\"_\"+aoi_outter['AOILabel'].astype(str)).astype(\"category\").cat.codes\n",
    "    \n",
    "    \n",
    "    list_var_to_encode=[\"AOILabel\",\"ComponentID\",\"FigureID_ComponentID\"]\n",
    "    for var in list_var_to_encode:\n",
    "        aoi_outter[var]=aoi_outter[var].astype('category').cat.codes\n",
    "\n",
    "    encoder=joblib.load(\"Cat_boost_step2_outer\")\n",
    "    AE_val=encoder.transform(aoi_outter[list_var_to_encode].copy())\n",
    "    AE_val.columns=[x+\"_encoded\" for x in AE_val.columns.tolist()]\n",
    "    aoi_outter=pd.concat([aoi_outter,AE_val],axis=1)\n",
    "    #col_features=[x for x in aoi_outter.columns.tolist() if \"_encoded\" in str(x)]\n",
    "    col_features=Features_to_bin+[x+\"_encoded\" for x in list_var_to_encode]+[\"Count_Pin\",\"Count_Pin_Figure\"]#+['MachineID_encoded']\n",
    "\n",
    "    xgb_cl=joblib.load(\"xgboost_step2_outter\")\n",
    "\n",
    "    X_test_final=aoi_outter[col_features].to_numpy()\n",
    "    aoi_outter['Pred']=xgb_cl.predict(X_test_final)\n",
    "\n",
    "    aoi=pd.concat([aoi_outter,aoi_inner])\n",
    "    \n",
    "    aoi_group=aoi.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])['Pred'].min().reset_index()\n",
    "    aoi_group2=aoi_group.copy()\n",
    "    aoi_group2.loc[aoi_group2['Pred']==0,\"Pred\"]=\"Bad\"\n",
    "    aoi_group2.loc[aoi_group['Pred']==1,'Pred']=\"Good\"\n",
    "\n",
    "    predicted=list(aoi_group2[[\"PanelID\",\"FigureID2\",\"ComponentID2\",'Pred']].itertuples(index=False, name=None))\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "strong-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first input is the SPI data in form of a Pandas DataFrame, exactly as it is read with pd.read_csv()\n",
    "# The second input is the AOI data. RepairLabel are not included, as you must predict it\n",
    "# The output must be the classification result. Each entry is a tuple (Panel, Figure, Component, PredictedRepairLabel)\n",
    "\n",
    "def classification_3 (spi, aoi):\n",
    "    \n",
    "    spi_real=spi.copy()\n",
    "    aoi_real=aoi.copy()\n",
    "\n",
    "    aoi_real['PinNumber']=aoi_real['PinNumber'].astype('Int64').astype(str)\n",
    "    aoi_real['FigureID2']=aoi_real['FigureID']\n",
    "    aoi_real['ComponentID2']=aoi_real['ComponentID']\n",
    "    aoi_real['Count_Pin']=aoi_real.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])[\"PinNumber\"].transform('count')\n",
    "    aoi_real['Count_Pin_Figure']=aoi_real.groupby([\"PanelID\",\"FigureID2\"])[\"PinNumber\"].transform('count')\n",
    "    aoi_real['Count_Pin_Panel']=aoi_real.groupby([\"PanelID\"])[\"PinNumber\"].transform('count')\n",
    "    \n",
    "    aoi_real[\"MachineID\"]=aoi_real[\"MachineID\"].astype('category').cat.codes\n",
    "\n",
    "    spi_real['FigureID2']=spi_real['FigureID']\n",
    "    spi_real['ComponentID2']=spi_real['ComponentID']\n",
    "    spi_real['key_spi']=(spi_real['PanelID'].astype(str)+'_'+spi_real['FigureID'].astype(int,errors='ignore').astype(str)+'_'+spi_real['ComponentID'].astype(str))\n",
    "    aoi_real['key_spi']=(aoi_real['PanelID'].astype(str)+'_'+aoi_real['FigureID'].astype(int,errors='ignore').astype(str)+'_'+aoi_real['ComponentID'].astype(str))\n",
    "\n",
    "    aoi_real=aoi_real.loc[aoi_real.OperatorLabel==\"Bad\"]\n",
    "\n",
    "    Features_to_bin=['Volume(%)',  'Area(%)', 'OffsetX(%)','OffsetY(%)',]\n",
    "    aoi_real['PanelID_FigureID_ComponentID']=aoi_real['PanelID'].astype(str)+'_'+aoi_real['FigureID'].astype(str)+'_'+aoi_real['ComponentID'].astype(str)\n",
    "    listAOI_Label=joblib.load('listAOI_Label')\n",
    "    for element in listAOI_Label:\n",
    "        aoi_real.loc[:,element]=0\n",
    "    for item in aoi_real['PanelID_FigureID_ComponentID'].unique().tolist():\n",
    "        list_item_aoilabel=[]\n",
    "        df_inter=aoi_real.loc[aoi_real['PanelID_FigureID_ComponentID']==item]\n",
    "        list_item_aoilabel=df_inter.AOILabel.unique()\n",
    "        for element in listAOI_Label:\n",
    "            if element in list_item_aoilabel:\n",
    "                aoi_real.loc[aoi_real['PanelID_FigureID_ComponentID']==item,element]=1\n",
    "    spi_real=spi_real.dropna(subset=['FigureID'])\n",
    "    aoi_real['FigureID_ComponentID']=(aoi_real['FigureID'].astype(str)+'_'+aoi_real['ComponentID'].astype(str)).astype(\"category\").cat.codes\n",
    "   \n",
    "    spi_real=spi_real.loc[spi_real['key_spi'].isin(aoi_real['key_spi'].unique().tolist())]\n",
    "    \n",
    "    Num_features=['Shape(um)',\n",
    "       'PosX(mm)', 'PosY(mm)','SizeX', 'SizeY']\n",
    "    spi_real['Shape(um)']=pd.to_numeric(spi_real['Shape(um)'])\n",
    "    \n",
    "    InputSPI_grouped=spi_real.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])[Features_to_bin+Num_features].mean().reset_index()\n",
    "    aoi_real=aoi_real.merge(InputSPI_grouped,how=\"left\",left_on=[\"PanelID\",\"FigureID2\",\"ComponentID2\"],right_on=[\"PanelID\",\"FigureID2\",\"ComponentID2\"])\n",
    "    aoi_real=aoi_real.drop_duplicates(subset=[\"PanelID\",\"FigureID2\",\"ComponentID2\"])\n",
    "    \n",
    "    \n",
    "    list_var_to_encode=[\"ComponentID\",\"FigureID_ComponentID\"]\n",
    "    for var in list_var_to_encode:\n",
    "        aoi_real[var]=aoi_real[var].astype('category').cat.codes\n",
    "    encoder=joblib.load(\"Cat_boost_step3_no_fusion\")\n",
    "\n",
    "    AE_val=encoder.transform(aoi_real[list_var_to_encode].copy())\n",
    "    AE_val.columns=[x+\"_encoded\" for x in AE_val.columns.tolist()]\n",
    "    X_test_final=pd.concat([aoi_real,AE_val],axis=1)\n",
    "    \n",
    "    \n",
    "    col_features=Num_features+[x+\"_encoded\" for x in list_var_to_encode]+[\"Count_Pin\",\"Count_Pin_Figure\",\"Count_Pin_Panel\"]+listAOI_Label+Features_to_bin+[\"MachineID\"]#+[x for x in X_val_final.columns.tolist() if \"_counts\" in x]#\n",
    "    X_test_final=X_test_final.replace(np.nan,0)\n",
    "    X_test_final=X_test_final[col_features].to_numpy()\n",
    "    \n",
    "    xgb_cl=joblib.load(\"xgboost_step3_no_fusion\")\n",
    "\n",
    "    aoi_real['Pred']=xgb_cl.predict(X_test_final)\n",
    "    \n",
    "    aoi_group=aoi_real.groupby([\"PanelID\",\"FigureID2\",\"ComponentID2\"])['Pred'].min().reset_index()\n",
    "    aoi_group.loc[aoi_group['Pred']==0,'Pred']=\"FalseScrap\"\n",
    "\n",
    "    aoi_group.loc[aoi_group['Pred']==1,'Pred']=\"NotPossibleToRepair\"\n",
    "\n",
    "    predicted=list(aoi_group[[\"PanelID\",\"FigureID2\",\"ComponentID2\",'Pred']].itertuples(index=False, name=None))\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-morocco",
   "metadata": {},
   "source": [
    "## Test the code\n",
    "\n",
    "In the following, we report a code that you can use to test if your script correctly handles the data.\n",
    "\n",
    "We will use a very similar piece of code to run your Notebook to build the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subtle-hormone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3156: DtypeWarning: Columns (17) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math\n",
    "import glob\n",
    "\n",
    "dfs = []\n",
    "for f in glob.glob(\"data/SPI_*.csv.zip\"):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "SPI = pd.concat(dfs)\n",
    "\n",
    "dfs = []\n",
    "for f in glob.glob(\"data/AOI_*.csv.zip\"):\n",
    "    dfs.append(pd.read_csv(f))\n",
    "AOI = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "increasing-static",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bigdatalab_cpu_202101/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score Task 1: 0.411160647197315\n",
      "F1 Score Task 2: 0.6576168929110106\n",
      "F1 Score Task 3: 0.9021846370683579\n",
      "Final Score: 0.6569873923922278\n"
     ]
    }
   ],
   "source": [
    "results_1 = set(classification_1(SPI.copy()))\n",
    "results_2 = classification_2(SPI, AOI[[\"PanelID\",\"FigureID\",\"MachineID\",\"ComponentID\",\"PinNumber\",\"AOILabel\"]])\n",
    "results_3 = classification_3(SPI, AOI[[\"PanelID\",\"FigureID\",\"MachineID\",\"ComponentID\",\"PinNumber\",\"AOILabel\",\"OperatorLabel\"]])\n",
    "\n",
    "# Performance Task 1\n",
    "groundtruth_1  = {tuple( [str(f) for f in e] ) for e in AOI[[\"PanelID\",\"FigureID\",\"ComponentID\"]].values}\n",
    "precision_1    = len(results_1&groundtruth_1)/len(results_1) if len(results_1) > 0 else 0\n",
    "recall_1       = len(results_1&groundtruth_1)/len(groundtruth_1) if len(groundtruth_1) > 0 else 0\n",
    "f1_1           = 2*precision_1*recall_1/(precision_1+recall_1) if precision_1+recall_1 > 0 else 0\n",
    "\n",
    "# Performance Task 2\n",
    "results_dict_2 = { (str(p), str(f), str(c)):l for p, f, c, l in results_2}\n",
    "validationdata_2 = []\n",
    "for t in AOI.drop_duplicates(subset=[\"PanelID\",\"FigureID\",\"ComponentID\"], keep=\"first\").itertuples():\n",
    "    predicted = results_dict_2.get(( str(t.PanelID), str(t.FigureID), str(t.ComponentID)), \"-\" )\n",
    "    validationdata_2.append((t.PanelID, t.FigureID, t.ComponentID, t.OperatorLabel, predicted))\n",
    "validationdata_2 = pd.DataFrame(validationdata_2, columns = [\"PanelID\",\"FigureID\",\"ComponentID\", \"Real\", \"Predicted\"]) \n",
    "f1_2 = classification_report(validationdata_2[\"Real\"], validationdata_2[\"Predicted\"],output_dict=True)[\"Bad\"][\"f1-score\"]\n",
    "\n",
    "# Performance Task 3\n",
    "results_dict_3 = { (str(p), str(f), str(c)):l for p, f, c, l in results_3}\n",
    "validationdata_3 = []\n",
    "for t in AOI[AOI[\"RepairLabel\"].isin({\"FalseScrap\",\"NotPossibleToRepair\"})]\\\n",
    "        .drop_duplicates(subset=[\"PanelID\",\"FigureID\",\"ComponentID\"], keep=\"first\").itertuples():\n",
    "    predicted = results_dict_3.get(( str(t.PanelID), str(t.FigureID), str(t.ComponentID)), \"-\" )\n",
    "    validationdata_3.append((t.PanelID, t.FigureID, t.ComponentID, t.RepairLabel, predicted))\n",
    "validationdata_3 = pd.DataFrame(validationdata_3, columns = [\"PanelID\",\"FigureID\",\"ComponentID\", \"Real\", \"Predicted\"]) \n",
    "cr = classification_report(validationdata_3[\"Real\"], validationdata_3[\"Predicted\"],output_dict=True)\n",
    "f1_3 = (cr[\"FalseScrap\"][\"f1-score\"] + cr[\"NotPossibleToRepair\"][\"f1-score\"])/2\n",
    "\n",
    "print(\"F1 Score Task 1:\", f1_1)\n",
    "print(\"F1 Score Task 2:\", f1_2)\n",
    "print(\"F1 Score Task 3:\", f1_3)\n",
    "print(\"Final Score:\", statistics.mean([f1_1, f1_2, f1_3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-configuration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
